{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
      "metadata": {
        "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697"
      },
      "source": [
        "# Bert baseline for POLAR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
      "metadata": {
        "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this part of the starter notebook, we will take you through the process of all three Subtasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OjMTlSH2RBI1",
      "metadata": {
        "id": "OjMTlSH2RBI1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4TFIhGFZPWDa",
      "metadata": {
        "id": "4TFIhGFZPWDa"
      },
      "outputs": [],
      "source": [
        "! pip install -q gdown\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir experiments\n",
        "! mkdir finetuned_models"
      ],
      "metadata": {
        "id": "xBYxURm-dTF-"
      },
      "id": "xBYxURm-dTF-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
      "metadata": {
        "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8IzshMEjO74s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IzshMEjO74s",
        "outputId": "242c7e60-1f2a-43e9-b9aa-26a84eb79e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tbAwUWN8X2JvXgdarjZ31f4XkqcpFVDk\n",
            "To: /content/dev_phase.zip\n",
            "100% 18.9M/18.9M [00:00<00:00, 117MB/s] \n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wHoKpZo8iMhHOm5TpvSS6Nr63Zk2w-P0\n",
            "To: /content/translated_subtasks.zip\n",
            "100% 1.93M/1.93M [00:00<00:00, 81.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "dev_phase_id = \"1tbAwUWN8X2JvXgdarjZ31f4XkqcpFVDk\"\n",
        "# subtask1_id = \"1q_I6dw9ZbCg3MbQ1wnC-419s2ocCyqaa\"\n",
        "# subtask2_id = \"1iHFDd_uihFi7vukWFq1hj32wfEH4dgBc\"\n",
        "# subtask3_id = \"1JA7_BbJDYORbmH06gWzz4-UhXgRBe1eI\"\n",
        "translated_tasks_id = \"1wHoKpZo8iMhHOm5TpvSS6Nr63Zk2w-P0\"\n",
        "\n",
        "! gdown --id $dev_phase_id\n",
        "! gdown --id $translated_tasks_id\n",
        "# ! gdown --id $subtask1_id\n",
        "# ! gdown --id $subtask2_id\n",
        "# ! gdown --id $subtask3_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5w0WSE89hdW",
      "metadata": {
        "id": "e5w0WSE89hdW"
      },
      "outputs": [],
      "source": [
        "! unzip dev_phase.zip\n",
        "! unzip translated_subtasks.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
      "metadata": {
        "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Module Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UkC2r47nManC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "UkC2r47nManC",
        "outputId": "3f87c52d-6b52-43ff-e5f5-10bc240952a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/cizixk1b?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f9e1de529f0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e749d08",
      "metadata": {
        "cellView": "form",
        "id": "8e749d08"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Class\n",
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
        "    self.texts=texts\n",
        "    self.labels=labels\n",
        "    self.tokenizer= tokenizer\n",
        "    self.max_length = max_length # Store max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
        "\n",
        "    # Ensure consistent tensor conversion for all items\n",
        "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O_XiSY3I5jSH",
      "metadata": {
        "id": "O_XiSY3I5jSH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Functions\n",
        "\n",
        "# Define metrics function\n",
        "# def compute_metrics(p):\n",
        "#     preds = np.argmax(p.predictions, axis=1)\n",
        "#     return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#     print(\"=\" * 50)\n",
        "#     print(f\"Type of p.predictions: {type(p.predictions)}\")\n",
        "#     print(f\"Type of p.label_ids: {type(p.label_ids)}\")\n",
        "\n",
        "#     if isinstance(p.predictions, tuple):\n",
        "#         print(f\"Predictions is tuple with {len(p.predictions)} elements\")\n",
        "#         for i, item in enumerate(p.predictions):\n",
        "#             print(f\"  Element {i}: {type(item)}, shape: {getattr(item, 'shape', 'no shape')}\")\n",
        "#         logits = p.predictions[0]\n",
        "#     elif isinstance(p.predictions, np.ndarray):\n",
        "#         print(f\"Predictions shape: {p.predictions.shape}\")\n",
        "#         logits = p.predictions\n",
        "#     else:\n",
        "#         print(f\"Predictions: {p.predictions}\")\n",
        "#         logits = np.array(p.predictions)\n",
        "\n",
        "#     print(f\"Label_ids shape: {p.label_ids.shape}\")\n",
        "#     print(f\"Logits shape after extraction: {logits.shape}\")\n",
        "\n",
        "#     preds = np.argmax(logits, axis=1)\n",
        "#     print(f\"Preds shape after argmax: {preds.shape}\")\n",
        "#     print(\"=\" * 50)\n",
        "\n",
        "#     return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# def compute_metrics(p):\n",
        "#     # Handle predictions\n",
        "#     logits = p.predictions\n",
        "#     preds = np.argmax(logits, axis=1)\n",
        "\n",
        "#     # Handle labels - convert tuple to array if needed\n",
        "#     if isinstance(p.label_ids, tuple):\n",
        "#         labels = np.array(p.label_ids)\n",
        "#     else:\n",
        "#         labels = p.label_ids\n",
        "\n",
        "#     # Flatten if needed\n",
        "#     if labels.ndim > 1:\n",
        "#         labels = labels.flatten()\n",
        "\n",
        "#     return {'f1_macro': f1_score(labels, preds, average='macro')}\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"Simple, robust compute_metrics function\"\"\"\n",
        "\n",
        "    # Predictions should be (n_samples, n_classes)\n",
        "    logits = p.predictions\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    # Labels should be (n_samples,)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Ensure labels are 1D\n",
        "    if isinstance(labels, (list, tuple)):\n",
        "        labels = np.array(labels)\n",
        "\n",
        "    if labels.ndim > 1:\n",
        "        labels = labels.flatten()\n",
        "\n",
        "    # Calculate F1\n",
        "    return {\n",
        "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "        'f1_weighted': f1_score(labels, preds, average='weighted')\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8RpSgNla3WQL",
      "metadata": {
        "cellView": "form",
        "id": "8RpSgNla3WQL"
      },
      "outputs": [],
      "source": [
        "# @title Experiment Class\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import yaml\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class Experiment:\n",
        "\n",
        "  def __init__(self, name, dir, description, baseline=None):\n",
        "    self.name = name\n",
        "    self.dir = dir\n",
        "    self.description = description\n",
        "    self.parameters = dict()\n",
        "    self.baseline = baseline\n",
        "\n",
        "  def update_param(self, parameter: 'Parameter'):\n",
        "    var_name = parameter.get_var_name()\n",
        "    parameter_class = parameter.get_parameter_class()\n",
        "    value = parameter.get_value()\n",
        "\n",
        "    assert isinstance(\n",
        "        value,\n",
        "        (\n",
        "            int, float, str, dict, list,\n",
        "            np.ndarray, torch.tensor\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if (parameter_class is None) or (parameter_class.lower() == 'global'):\n",
        "      self.parameters[var_name] = value\n",
        "      return\n",
        "\n",
        "    if parameter_class not in self.parameters:\n",
        "      self.parameters[parameter_class] = dict()\n",
        "    self.parameters[parameter_class][var_name] = value\n",
        "\n",
        "  def save(self):\n",
        "    experiment_dict = {\n",
        "        'name': self.name,\n",
        "        'baseline': self.baseline,\n",
        "        'description': self.description,\n",
        "        'parameters': self.parameters,\n",
        "    }\n",
        "\n",
        "    with open(self.dir, \"w\") as f:\n",
        "      yaml.dump(experiment_dict, f, default_flow_style=False)\n",
        "\n",
        "    print(f\"Model saved to {self.dir}\")\n",
        "\n",
        "  def add_params(self, parameters: List['Parameter']):\n",
        "\n",
        "    for parameter in parameters:\n",
        "      self.update_param(parameter)\n",
        "\n",
        "\n",
        "class Parameter:\n",
        "\n",
        "  def __init__(self, value, var_name, parameter_class):\n",
        "    self.__var_name = var_name\n",
        "    self.__value = value\n",
        "    self.__parameter_class = parameter_class\n",
        "\n",
        "  def get_var_name(self):\n",
        "    return self.__var_name\n",
        "\n",
        "  def get_parameter_class(self):\n",
        "    return self.__parameter_class\n",
        "\n",
        "  def get_value(self):\n",
        "    return self.__value\n",
        "\n",
        "  def set_value(self, value):\n",
        "    self.__value = value\n",
        "\n",
        "  def set_var_name(self, var_name):\n",
        "    self.__var_name = var_name\n",
        "\n",
        "  def set_parameter_class(self, parameter_class):\n",
        "    self.__parameter_class = parameter_class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiTask Trainer Classes"
      ],
      "metadata": {
        "id": "yMxw5Vozdb_N"
      },
      "id": "yMxw5Vozdb_N"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PolarPairs Approach\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer\n",
        "import numpy as np\n",
        "\n",
        "class PolarPairsDataset(Dataset):\n",
        "    \"\"\"Combined dataset with stratified target sampling\"\"\"\n",
        "    def __init__(self, source_texts, target_texts, source_labels, target_labels,\n",
        "                 tokenizer, max_length=128, subset_size=20):\n",
        "        self.source_labels = source_labels\n",
        "        self.target_labels = np.array(target_labels)\n",
        "        self.source_texts = source_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "        self.max_length = max_length\n",
        "        self.subset_size = subset_size\n",
        "\n",
        "        # Pre-compute indices by class for stratified sampling\n",
        "        self.pos_indices = np.where(self.target_labels == 1)[0]\n",
        "        self.neg_indices = np.where(self.target_labels == 0)[0]\n",
        "\n",
        "        print(f\"Target distribution - Positive: {len(self.pos_indices)}, \"\n",
        "              f\"Negative: {len(self.neg_indices)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = str(self.source_texts[idx])\n",
        "        source_label = int(self.source_labels[idx])\n",
        "\n",
        "        # Stratified sampling: ensure balanced positive/negative targets\n",
        "        n_pos = self.subset_size // 2\n",
        "        n_neg = self.subset_size - n_pos\n",
        "\n",
        "        # Sample with replacement if needed\n",
        "        pos_sample = np.random.choice(self.pos_indices, size=n_pos,\n",
        "                                     replace=len(self.pos_indices) < n_pos)\n",
        "        neg_sample = np.random.choice(self.neg_indices, size=n_neg,\n",
        "                                     replace=len(self.neg_indices) < n_neg)\n",
        "\n",
        "        target_indices = np.concatenate([pos_sample, neg_sample])\n",
        "        np.random.shuffle(target_indices)\n",
        "\n",
        "        target_texts_batch = [str(self.target_texts[i]) for i in target_indices]\n",
        "        target_labels_batch = [int(self.target_labels[i]) for i in target_indices]\n",
        "\n",
        "        # Encode\n",
        "        source_encoding = self.tokenizer(\n",
        "            source, max_length=self.max_length, padding='max_length',\n",
        "            truncation=True, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            target_texts_batch, max_length=self.max_length, padding='max_length',\n",
        "            truncation=True, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'x_input_ids': source_encoding['input_ids'].squeeze(0),\n",
        "            'x_attention_mask': source_encoding['attention_mask'].squeeze(0),\n",
        "            'x_hat_input_ids': target_encoding['input_ids'],\n",
        "            'x_hat_attention_mask': target_encoding['attention_mask'],\n",
        "            'polar_labels': torch.tensor(source_label, dtype=torch.long),\n",
        "            'hat_labels': torch.tensor(target_labels_batch, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class MultiLingualPolarPairsAlignment(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder_model_name, pretrained_encoder_name, num_labels, alignment_normalization=True):\n",
        "    super(MultiLingualPolarPairsAlignment, self).__init__()\n",
        "\n",
        "    self.encoder = AutoModel.from_pretrained(pretrained_encoder_name)\n",
        "    self.encoder.train()\n",
        "    self.pretrained_encoder = AutoModel.from_pretrained(pretrained_encoder_name)\n",
        "    self.pretrained_encoder.eval()\n",
        "\n",
        "    encoder_config = self.encoder.config\n",
        "    embedding_size = encoder_config.hidden_size\n",
        "\n",
        "    self.alignment_head = nn.Linear(embedding_size, embedding_size)\n",
        "    self.classification_head = nn.Linear(embedding_size, num_labels)\n",
        "    self.alignment_layer_norm = nn.LayerNorm(embedding_size)\n",
        "    self.alignment_normalization = alignment_normalization\n",
        "\n",
        "  def forward(self, x_input_ids, x_attention_mask, x_hat_input_ids, x_hat_attention_mask, polar_labels, hat_labels):\n",
        "    # FIX 1: Extract last_hidden_state\n",
        "    x1_hidden = self.encoder(\n",
        "        input_ids=x_input_ids,\n",
        "        attention_mask=x_attention_mask\n",
        "    ).last_hidden_state  # ✓ Shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "    # FIX 2: Flatten targets before encoding\n",
        "    batch_size, subset_size, seq_len = x_hat_input_ids.shape\n",
        "\n",
        "    x_hat_input_ids_flat = x_hat_input_ids.view(batch_size * subset_size, seq_len)\n",
        "    x_hat_attention_mask_flat = x_hat_attention_mask.view(batch_size * subset_size, seq_len)\n",
        "\n",
        "    x2_hidden_flat = self.pretrained_encoder(\n",
        "        input_ids=x_hat_input_ids_flat,\n",
        "        attention_mask=x_hat_attention_mask_flat\n",
        "    ).last_hidden_state  # ✓ Shape: (batch_size * subset_size, seq_len, hidden_size)\n",
        "\n",
        "    # FIX 3: Reshape back correctly\n",
        "    hidden_size = x2_hidden_flat.size(-1)\n",
        "    x2_hidden = x2_hidden_flat.view(batch_size, subset_size, seq_len, hidden_size)\n",
        "\n",
        "    # Extract CLS tokens\n",
        "    x1_cls = x1_hidden[:, 0, :]  # (batch_size, hidden_size)\n",
        "    x2_cls = x2_hidden[:, :, 0, :]  # (batch_size, subset_size, hidden_size)\n",
        "\n",
        "    # Apply alignment head\n",
        "    # x1_aligned = F.leaky_relu(self.alignment_head(x1_cls))  # (batch_size, hidden_size)\n",
        "    x1_aligned = self.alignment_head(x1_cls) + x1_cls  # (batch_size, hidden_size)\n",
        "    if self.alignment_normalization:\n",
        "      x1_aligned = self.alignment_layer_norm(x1_cls + x1_aligned)\n",
        "\n",
        "    # # Apply alignment to all target embeddings\n",
        "    # batch_size, subset_size, hidden_size = x2_cls.shape\n",
        "    # x2_cls_flat = x2_cls.view(batch_size * subset_size, hidden_size)\n",
        "    # x2_aligned_flat = F.leaky_relu(self.alignment_head(x2_cls_flat)) + x2_cls_flat\n",
        "    # if self.alignment_normalization:\n",
        "    #   x2_aligned_flat = self.alignment_layer_norm(x2_cls_flat + x2_aligned_flat)\n",
        "    # x2_aligned = x2_aligned_flat.view(batch_size, subset_size, hidden_size)\n",
        "\n",
        "    # Use target embeddings directly (no transformation)\n",
        "    x2_aligned = x2_cls  # (batch_size, subset_size, hidden_size)\n",
        "\n",
        "    # Classification logits (only from source)\n",
        "    logits = self.classification_head(x1_aligned)\n",
        "\n",
        "    return logits, x1_aligned, x2_aligned\n",
        "\n",
        "\n",
        "def polar_pairs_contrastive_loss(\n",
        "    logits: torch.Tensor,\n",
        "    x1_aligned: torch.Tensor,\n",
        "    x2_aligned: torch.Tensor,\n",
        "    polar_labels: torch.Tensor,\n",
        "    hat_labels: torch.Tensor,\n",
        "    lambda_align: float = 0.5,\n",
        "    temperature: float = 0.07\n",
        ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Improved loss with:\n",
        "    1. Classification loss\n",
        "    2. Contrastive alignment loss (pull same class together, push different apart)\n",
        "    3. Class balance handling\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Classification loss with class weights\n",
        "    num_classes = logits.size(1)  # Get number of classes from logits\n",
        "    class_counts = torch.bincount(polar_labels, minlength=num_classes)\n",
        "    class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "    # Move weights to same device as logits\n",
        "    class_weights = class_weights.to(logits.device)\n",
        "\n",
        "    classification_loss = F.cross_entropy(logits, polar_labels, weight=class_weights)\n",
        "\n",
        "    # 2. Contrastive alignment loss\n",
        "    batch_size, subset_size, hidden_size = x2_aligned.shape\n",
        "\n",
        "    # Expand source embeddings to match all targets\n",
        "    x1_expanded = x1_aligned.unsqueeze(1).expand(-1, subset_size, -1)\n",
        "    x1_flat = x1_expanded.reshape(batch_size * subset_size, hidden_size)\n",
        "    x2_flat = x2_aligned.reshape(batch_size * subset_size, hidden_size)\n",
        "\n",
        "    # Expand source labels to match\n",
        "    source_labels_expanded = polar_labels.unsqueeze(1).expand(-1, subset_size)\n",
        "    source_labels_flat = source_labels_expanded.reshape(-1)\n",
        "    target_labels_flat = hat_labels.reshape(-1)\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    x1_norm = F.normalize(x1_flat, p=2, dim=1)\n",
        "    x2_norm = F.normalize(x2_flat, p=2, dim=1)\n",
        "    similarity = torch.mm(x1_norm, x2_norm.t()) / temperature\n",
        "\n",
        "    # Create masks for positive and negative pairs\n",
        "    labels_equal = (source_labels_flat.unsqueeze(1) == target_labels_flat.unsqueeze(0))\n",
        "\n",
        "    # InfoNCE-style contrastive loss\n",
        "    exp_sim = torch.exp(similarity)\n",
        "\n",
        "    # Mask out self-similarities (diagonal)\n",
        "    mask_self = torch.eye(similarity.size(0), device=similarity.device).bool()\n",
        "    exp_sim = exp_sim.masked_fill(mask_self, 0)\n",
        "\n",
        "    # Positive pairs: same label\n",
        "    pos_sim = (exp_sim * labels_equal.float()).sum(dim=1)\n",
        "\n",
        "    # All pairs (excluding self)\n",
        "    all_sim = exp_sim.sum(dim=1)\n",
        "\n",
        "    # Contrastive loss: -log(pos / (pos + neg))\n",
        "    alignment_loss = -torch.log((pos_sim + 1e-8) / (all_sim + 1e-8))\n",
        "    alignment_loss = alignment_loss.mean()\n",
        "\n",
        "    # 3. Total loss\n",
        "    total_loss = (1.0 - lambda_align) * classification_loss + lambda_align * alignment_loss\n",
        "\n",
        "    return total_loss, classification_loss, alignment_loss\n",
        "\n",
        "\n",
        "class PolarPairsTrainer(Trainer):\n",
        "    def __init__(self, lambda_align=0.5, temperature=0.07, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lambda_align = lambda_align\n",
        "        self.temperature = temperature\n",
        "        self.label_names = ['polar_labels', 'hat_labels']\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        logits, x1_aligned, x2_aligned = model(\n",
        "            x_input_ids=inputs['x_input_ids'],\n",
        "            x_attention_mask=inputs['x_attention_mask'],\n",
        "            x_hat_input_ids=inputs['x_hat_input_ids'],\n",
        "            x_hat_attention_mask=inputs['x_hat_attention_mask'],\n",
        "            polar_labels=inputs['polar_labels'],\n",
        "            hat_labels=inputs['hat_labels']\n",
        "        )\n",
        "\n",
        "        total_loss, classification_loss, alignment_loss = polar_pairs_contrastive_loss(\n",
        "            logits=logits,\n",
        "            x1_aligned=x1_aligned,\n",
        "            x2_aligned=x2_aligned,\n",
        "            polar_labels=inputs['polar_labels'],\n",
        "            hat_labels=inputs['hat_labels'],\n",
        "            lambda_align=self.lambda_align,\n",
        "            temperature=self.temperature\n",
        "        )\n",
        "\n",
        "        # Log component losses\n",
        "        if self.state.global_step % 10 == 0:\n",
        "            self.log({\n",
        "                'classification_loss': classification_loss.item(),\n",
        "                'alignment_loss': alignment_loss.item(),\n",
        "                'lambda': self.lambda_align\n",
        "            })\n",
        "\n",
        "        if return_outputs:\n",
        "            outputs = {'logits': logits}\n",
        "            return total_loss, outputs\n",
        "        else:\n",
        "            return total_loss\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        with torch.no_grad():\n",
        "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
        "            logits = outputs['logits']\n",
        "\n",
        "        labels = inputs['polar_labels'].detach().cpu()\n",
        "\n",
        "        if prediction_loss_only:\n",
        "            return (loss.detach(), None, None)\n",
        "\n",
        "        return (loss.detach(), logits.detach().cpu(), labels)\n",
        "\n",
        "\n",
        "# Enhanced collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "@dataclass\n",
        "class PolarPairsCollator:\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        batch = {}\n",
        "\n",
        "        batch['x_input_ids'] = torch.stack([f['x_input_ids'] for f in features])\n",
        "        batch['x_attention_mask'] = torch.stack([f['x_attention_mask'] for f in features])\n",
        "        batch['x_hat_input_ids'] = torch.stack([f['x_hat_input_ids'] for f in features])\n",
        "        batch['x_hat_attention_mask'] = torch.stack([f['x_hat_attention_mask'] for f in features])\n",
        "\n",
        "        polar_labels = []\n",
        "        for f in features:\n",
        "            label = f['polar_labels']\n",
        "            if isinstance(label, torch.Tensor):\n",
        "                polar_labels.append(label.item() if label.dim() == 0 else label[0].item())\n",
        "            else:\n",
        "                polar_labels.append(int(label))\n",
        "\n",
        "        batch['polar_labels'] = torch.tensor(polar_labels, dtype=torch.long)\n",
        "        batch['hat_labels'] = torch.stack([f['hat_labels'] for f in features])\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "7xrhL4tOOogt"
      },
      "id": "7xrhL4tOOogt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MT5\n",
        "\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    \"\"\"Combined dataset for classification and translation\"\"\"\n",
        "    def __init__(self, texts, labels, source_texts, target_texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.source_texts = source_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Classification inputs\n",
        "        text = str(self.texts[idx])\n",
        "        class_encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Translation inputs\n",
        "        source = str(self.source_texts[idx])\n",
        "        target = str(self.target_texts[idx])\n",
        "\n",
        "        trans_source_encoding = self.tokenizer(\n",
        "            source,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        trans_target_encoding = self.tokenizer(\n",
        "            target,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        translation_labels = trans_target_encoding['input_ids'].squeeze(0)\n",
        "        translation_labels[translation_labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': class_encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': class_encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(int(self.labels[idx]), dtype=torch.long),\n",
        "            'translation_input_ids': trans_source_encoding['input_ids'].squeeze(0),\n",
        "            'translation_attention_mask': trans_source_encoding['attention_mask'].squeeze(0),\n",
        "            'translation_labels': translation_labels\n",
        "        }\n",
        "\n",
        "\n",
        "class MultiTaskMT5(nn.Module):\n",
        "    \"\"\"mT5 model for both classification and translation\"\"\"\n",
        "    def __init__(self, model_name='google/mt5-small', num_labels=3):\n",
        "        super(MultiTaskMT5, self).__init__()\n",
        "\n",
        "        # Load pretrained mT5\n",
        "        self.mt5 = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.config = self.mt5.config\n",
        "\n",
        "        # Classification head (uses encoder output)\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(self.config.d_model, num_labels)\n",
        "        )\n",
        "\n",
        "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "        \"\"\"Enable gradient checkpointing for the underlying mT5 model\"\"\"\n",
        "        self.mt5.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"Disable gradient checkpointing for the underlying mT5 model\"\"\"\n",
        "        self.mt5.gradient_checkpointing_disable()\n",
        "\n",
        "    def freeze_base_model(self):\n",
        "        \"\"\"Freeze the mT5 encoder and decoder, only train the classification head\"\"\"\n",
        "        print(\"Freezing mT5 base model (encoder + decoder)...\")\n",
        "\n",
        "        # Freeze encoder\n",
        "        for param in self.mt5.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Freeze decoder\n",
        "        for param in self.mt5.decoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Keep lm_head (language modeling head) trainable for translation\n",
        "        # This is the final projection layer that outputs vocabulary logits\n",
        "        for param in self.mt5.lm_head.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Classification head is trainable by default (not frozen)\n",
        "\n",
        "        # Print trainable parameters\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids=None, attention_mask=None, labels=None,\n",
        "                translation_input_ids=None, translation_attention_mask=None,\n",
        "                translation_labels=None):\n",
        "\n",
        "        classification_loss = None\n",
        "        classification_logits = None\n",
        "\n",
        "        # Classification task\n",
        "        if input_ids is not None:\n",
        "            encoder_outputs = self.mt5.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            hidden_states = encoder_outputs.last_hidden_state\n",
        "            pooled_output = (hidden_states * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
        "            classification_logits = self.classification_head(pooled_output)\n",
        "\n",
        "            if labels is not None:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                classification_loss = loss_fct(classification_logits, labels)\n",
        "\n",
        "        # Translation task\n",
        "        translation_loss = None\n",
        "        translation_logits = None\n",
        "\n",
        "        if translation_input_ids is not None and translation_labels is not None:\n",
        "            translation_outputs = self.mt5(\n",
        "                input_ids=translation_input_ids,\n",
        "                attention_mask=translation_attention_mask,\n",
        "                labels=translation_labels\n",
        "            )\n",
        "            translation_loss = translation_outputs.loss\n",
        "            translation_logits = translation_outputs.logits\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = None\n",
        "        if classification_loss is not None and translation_loss is not None:\n",
        "            total_loss = classification_loss + translation_loss\n",
        "        elif classification_loss is not None:\n",
        "            total_loss = classification_loss\n",
        "        elif translation_loss is not None:\n",
        "            total_loss = translation_loss\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss,\n",
        "            'classification_loss': classification_loss,\n",
        "            'translation_loss': translation_loss,\n",
        "            'logits': classification_logits,\n",
        "            'classification_logits': classification_logits,\n",
        "            'translation_logits': translation_logits\n",
        "        }\n",
        "\n",
        "class MultiTaskTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"\n",
        "        Compute loss for multi-task learning.\n",
        "        Works with BERT, mT5, mBART, or any custom model.\n",
        "        \"\"\"\n",
        "        outputs = model(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            translation_input_ids=inputs.get('translation_input_ids'),\n",
        "            translation_attention_mask=inputs.get('translation_attention_mask'),\n",
        "            labels=inputs.get('labels'),\n",
        "            translation_labels=inputs.get('translation_labels')\n",
        "        )\n",
        "\n",
        "        loss = outputs['loss']\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "LrOomH6NdadH",
        "cellView": "form"
      },
      "id": "LrOomH6NdadH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aIt64l96d4TR",
      "metadata": {
        "id": "aIt64l96d4TR"
      },
      "source": [
        "# Subtask 1 - Polarization detection\n",
        "\n",
        "This is a binary classification to determine whether a post contains polarized content (Polarized or Not Polarized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06pBeSoa7zYv",
      "metadata": {
        "id": "06pBeSoa7zYv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Experiment Definition & Parameter Setting\n",
        "\n",
        "import os\n",
        "\n",
        "languages = ['amh', 'eng', 'hau', 'swa'] # @param\n",
        "languages_param = Parameter(languages, 'language', None)\n",
        "\n",
        "model_name = \"microsoft/deberta-v3-small\" # @param {type: \"string\"}\n",
        "model_param = Parameter(model_name, 'model_name', None)\n",
        "\n",
        "tokenizer_choice = \"microsoft/deberta-v3-small\" # @param {type:\"string\"}\n",
        "tokenizer_param = Parameter(tokenizer_choice, 'tokenizer', 'Preprocessing')\n",
        "\n",
        "n_labels = 2 # @param\n",
        "n_labels_param = Parameter(n_labels, 'n_labels', None)\n",
        "\n",
        "max_length = 128 # @param\n",
        "max_length_param = Parameter(max_length, 'max_length', 'Hyperparameter')\n",
        "\n",
        "num_epochs = 3 # @param\n",
        "epochs_param = Parameter(num_epochs, 'epochs', 'Hyperparameter')\n",
        "\n",
        "lr = 1e-4 # @param\n",
        "lr_param = Parameter(lr, 'learning_rate', 'Hyperparameter')\n",
        "\n",
        "train_batch = 8 # @param\n",
        "train_batch_param = Parameter(train_batch, 'train_batch_size', 'Hyperparameter')\n",
        "\n",
        "eval_batch = 8 # @param\n",
        "eval_batch_param = Parameter(eval_batch, 'eval_batch_size', 'Hyperparameter')\n",
        "\n",
        "eval_strategy = \"epoch\" # @param {type: \"string\"}\n",
        "eval_strategy_param = Parameter(eval_strategy, 'eval_strategy', 'Hyperparameter')\n",
        "\n",
        "experiment_version = \"v1.1.0\" # @param {type: \"string\"}\n",
        "experiment_dir = f\"experiments/{experiment_version}.yaml\"\n",
        "if os.path.exists(experiment_dir):\n",
        "  raise ValueError(f\"Experiment {experiment_version} already exists\")\n",
        "experiment_description = \"Contrastive Polar Pairs Alignment using microsoft-deberta-v3-small\" # @param {type: \"string\"}\n",
        "experiment_baseline = \"v1.0.0\" # @param {type: \"string\"}\n",
        "experiment = Experiment(\n",
        "    experiment_version,\n",
        "    experiment_dir,\n",
        "    experiment_description,\n",
        "    experiment_baseline\n",
        ")\n",
        "experiment.add_params([\n",
        "    languages_param, model_param, max_length_param,\n",
        "    tokenizer_param, epochs_param, lr_param,\n",
        "    train_batch_param, eval_batch_param,\n",
        "    eval_strategy_param\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JClhMb_kn-uQ",
      "metadata": {
        "id": "JClhMb_kn-uQ"
      },
      "source": [
        "## Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c97269",
      "metadata": {
        "id": "21c97269"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=epochs_param.get_value(),\n",
        "    per_device_train_batch_size=train_batch_param.get_value(),  # mT5-small can handle 2-4\n",
        "    per_device_eval_batch_size=eval_batch_param.get_value(),\n",
        "    gradient_accumulation_steps=8,\n",
        "    eval_strategy=eval_strategy_param.get_value(),\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=0,\n",
        "    load_best_model_at_end=False,\n",
        "    eval_accumulation_steps=1,\n",
        "    gradient_checkpointing=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Loading and Splitting\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "def load_and_split_bilingual_data(subtask, source_lang, target_lang, test_size=0.2, random_state=42, verbose=True):\n",
        "    \"\"\"\n",
        "    Load and split bilingual polarization data with stratified sampling.\n",
        "\n",
        "    Args:\n",
        "        subtask (str): Subtask name (e.g., 'subtask1', 'subtask2')\n",
        "        source_lang (str): Source language code (e.g., 'swa', 'eng')\n",
        "        target_lang (str): Target language code (e.g., 'eng', 'swa')\n",
        "        test_size (float): Proportion of validation set (default: 0.2)\n",
        "        random_state (int): Random seed for reproducibility (default: 42)\n",
        "        verbose (bool): Print distribution statistics (default: True)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_df, val_df) with columns:\n",
        "            - source_text\n",
        "            - polarization (source label)\n",
        "            - target_text\n",
        "            - target_polarization (target label)\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the DataFrames\n",
        "    source = pd.read_csv(f'{subtask}/train/{source_lang}.csv')\n",
        "    target = pd.read_csv(f'{subtask}/train/{target_lang}.csv')\n",
        "\n",
        "    # --- 1. Shape Matching and Alignment ---\n",
        "    source_len = source.shape[0]\n",
        "    target_len = target.shape[0]\n",
        "\n",
        "    if target_len < source_len:\n",
        "        # Case A: Target is shorter than Source (Repeat/Tile the Target)\n",
        "        repeat_factor = math.ceil(source_len / target_len)\n",
        "        target_aligned = pd.concat([target] * repeat_factor, ignore_index=True).iloc[0:source_len]\n",
        "\n",
        "    elif target_len > source_len:\n",
        "        # Case B: Target is longer than Source (Truncate the Target)\n",
        "        target_aligned = target.iloc[0:source_len]\n",
        "\n",
        "    else:\n",
        "        # Case C: Target and Source are already the same length\n",
        "        target_aligned = target.copy()\n",
        "\n",
        "    # --- 2. Data Combination ---\n",
        "    data = pd.DataFrame({\n",
        "        'source_text': source['text'],\n",
        "        'polarization': source['polarization'],\n",
        "        'target_text': target_aligned['text'],\n",
        "        'target_polarization': target_aligned['polarization']\n",
        "    })\n",
        "\n",
        "    # --- 3. Stratified Splitting ---\n",
        "    sss_source = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Split based on source polarization\n",
        "    for train_index, val_index in sss_source.split(data, data['polarization']):\n",
        "        train = data.iloc[train_index].reset_index(drop=True)\n",
        "        val = data.iloc[val_index].reset_index(drop=True)\n",
        "\n",
        "    # Stratified split for target language in validation set\n",
        "    full_target_pool = data[['target_text', 'target_polarization']].copy()\n",
        "    val_size = val.shape[0]\n",
        "    test_ratio = val_size / data.shape[0]\n",
        "\n",
        "    sss_target = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=random_state)\n",
        "    for _, new_val_target_index in sss_target.split(full_target_pool, full_target_pool['target_polarization']):\n",
        "        new_val_target = full_target_pool.iloc[new_val_target_index].reset_index(drop=True)\n",
        "\n",
        "    # Verify and update validation target data\n",
        "    if new_val_target.shape[0] == val.shape[0]:\n",
        "        val['target_text'] = new_val_target['target_text'].values\n",
        "        val['target_polarization'] = new_val_target['target_polarization'].values\n",
        "    else:\n",
        "        raise ValueError(f\"Stratified target split size ({new_val_target.shape[0]}) \"\n",
        "                        f\"does not match validation set size ({val.shape[0]}).\")\n",
        "\n",
        "    # --- 4. Print Results (if verbose) ---\n",
        "    if verbose:\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Dataset: {subtask} | {source_lang} → {target_lang}\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total samples: {data.shape[0]}\")\n",
        "        print(f\"Train set size: {train.shape[0]} ({train.shape[0]/data.shape[0]*100:.1f}%)\")\n",
        "        print(f\"Validation set size: {val.shape[0]} ({val.shape[0]/data.shape[0]*100:.1f}%)\")\n",
        "\n",
        "        print(\"\\n--- Polarization Distribution ---\")\n",
        "        print(\"\\nTrain Set (Source):\")\n",
        "        print(train['polarization'].value_counts().sort_index())\n",
        "        print(f\"  Ratio: {train['polarization'].value_counts(normalize=True).sort_index().to_dict()}\")\n",
        "\n",
        "        print(\"\\nValidation Set (Source):\")\n",
        "        print(val['polarization'].value_counts().sort_index())\n",
        "        print(f\"  Ratio: {val['polarization'].value_counts(normalize=True).sort_index().to_dict()}\")\n",
        "\n",
        "        print(\"\\nValidation Set (Target):\")\n",
        "        print(val['target_polarization'].value_counts().sort_index())\n",
        "        print(f\"  Ratio: {val['target_polarization'].value_counts(normalize=True).sort_index().to_dict()}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    return train, val"
      ],
      "metadata": {
        "id": "W7rBF0GUXvlg"
      },
      "id": "W7rBF0GUXvlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source = pd.read_csv(f'subtask1/train/amh.csv')\n",
        "# target = pd.read_csv(f'subtask1/train/eng.csv')\n",
        "\n",
        "# train_stop_idx = int(0.8*source.shape[0])\n",
        "\n",
        "# data = pd.DataFrame()\n",
        "# data['source_text'] = source['text']\n",
        "# data['polarization'] = source['polarization']\n",
        "# data['target_text'] = target['text']\n",
        "# data['target_polarization'] = target['polarization']\n",
        "\n",
        "# train = data[0: train_stop_idx]\n",
        "# val = data[train_stop_idx: ]"
      ],
      "metadata": {
        "id": "fYapHwvWSAdH"
      },
      "id": "fYapHwvWSAdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JcRDLDV2d2QA",
      "metadata": {
        "id": "JcRDLDV2d2QA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Search for Base Model (English Prioritized)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "for language in languages_param.get_value():\n",
        "  time.sleep(0.25)\n",
        "  clear_output()\n",
        "\n",
        "  # Load the tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_param.get_value())\n",
        "\n",
        "  train, val = load_and_split_bilingual_data(\n",
        "      subtask = 'subtask1',\n",
        "      source_lang = language,\n",
        "      target_lang = 'eng'\n",
        "  )\n",
        "\n",
        "  # Create datasets\n",
        "  train_dataset = PolarizationDataset(train['source_text'].tolist(), train['polarization'].tolist(), tokenizer)\n",
        "  val_dataset = PolarizationDataset(val['source_text'].tolist(), val['polarization'].tolist(), tokenizer)\n",
        "\n",
        "  # Load the model\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(tokenizer_param.get_value(), num_labels=n_labels_param.get_value())\n",
        "\n",
        "  # Initialize the Trainer\n",
        "  trainer = Trainer(\n",
        "      model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "      args=training_args,                  # training arguments, defined above\n",
        "      train_dataset=train_dataset,         # training dataset\n",
        "      eval_dataset=val_dataset,            # evaluation dataset\n",
        "      compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        "      data_collator=DataCollatorWithPadding(tokenizer) # Data collator for dynamic padding\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  trainer.train()\n",
        "\n",
        "  eval_results = trainer.evaluate()\n",
        "  print(f\"Macro F1 score on {language} validation set: {eval_results['eval_f1_macro']}\")\n",
        "\n",
        "  # ===== SAVE THE FINE-TUNED MODEL =====\n",
        "  save_path = f'finetuned_models/{language}_{tokenizer_param.get_value()}'\n",
        "  model.save_pretrained(save_path)\n",
        "  tokenizer.save_pretrained(save_path)\n",
        "  print(f\"Saved {language} model to {save_path}\")\n",
        "  # ====================================\n",
        "\n",
        "  eval_results_param = Parameter(eval_results, f\"{language}_eval_results\", \"Performance\")\n",
        "  experiment.add_params([eval_results_param])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Update Parameters 4 Contrastive Alignment\n",
        "\n",
        "num_epochs = 3 # @param\n",
        "epochs_param = Parameter(num_epochs, 'epochs', 'Hyperparameter')\n",
        "\n",
        "train_batch = 8 # @param\n",
        "train_batch_param = Parameter(train_batch, 'train_batch_size', 'Hyperparameter')\n",
        "\n",
        "eval_batch = 8 # @param\n",
        "eval_batch_param = Parameter(eval_batch, 'eval_batch_size', 'Hyperparameter')\n",
        "\n",
        "contrastive_subset = 2 # @param\n",
        "contrastive_subset_param = Parameter(\n",
        "    contrastive_subset,\n",
        "    'subset_size',\n",
        "    'Hyperparameter'\n",
        ")\n",
        "\n",
        "contrastive_temperature = 0.07 # @param\n",
        "contrastive_temperature_param = Parameter(\n",
        "    contrastive_temperature,\n",
        "    'tau',\n",
        "    'Hyperparameter'\n",
        ")\n",
        "\n",
        "lambda_align = 0.1 # @param\n",
        "lambda_align_param = Parameter(\n",
        "    lambda_align,\n",
        "    'lambda',\n",
        "    'Hyperparameter'\n",
        ")\n",
        "\n",
        "encoder_lr = 1e-4 # @param\n",
        "encoder_lr_param = Parameter(\n",
        "    encoder_lr,\n",
        "    'encoder_lr',\n",
        "    'Hyperparameter'\n",
        ")\n",
        "\n",
        "alignment_lr = 5e-3 # @param\n",
        "alignment_lr_param = Parameter(\n",
        "    alignment_lr,\n",
        "    'alignment_lr',\n",
        "    'Hyperparameter'\n",
        ")\n",
        "\n",
        "classification_lr = 5e-3 # @param\n",
        "classification_lr_param = Parameter(\n",
        "    classification_lr,\n",
        "    'encoder_lr',\n",
        "    'Hyperparameter'\n",
        ")\n",
        "\n",
        "alignment_normalization = False # @param {type: 'boolean'}\n",
        "alignment_normalization_param = Parameter(\n",
        "    alignment_normalization,\n",
        "    'alignment_normalization',\n",
        "    'Hyperparameter'\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pbuhOAJF2UcH"
      },
      "id": "pbuhOAJF2UcH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Update Training Arguments\n",
        "\n",
        "# Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=epochs_param.get_value(),\n",
        "    per_device_train_batch_size=train_batch_param.get_value(),  # mT5-small can handle 2-4\n",
        "    per_device_eval_batch_size=eval_batch_param.get_value(),\n",
        "    gradient_accumulation_steps=8,\n",
        "    eval_strategy=eval_strategy_param.get_value(),\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=0,\n",
        "    load_best_model_at_end=False,\n",
        "    eval_accumulation_steps=1,\n",
        "    gradient_checkpointing=False,\n",
        ")"
      ],
      "metadata": {
        "id": "zhwQHpLRueyD"
      },
      "id": "zhwQHpLRueyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PolarPairsAlignment Training\n",
        "\n",
        "from torch.optim import AdamW\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "SRC_LANG = 'swa'\n",
        "TGT_LANG = 'eng'\n",
        "\n",
        "train, val = load_and_split_bilingual_data(\n",
        "      subtask = 'subtask1',\n",
        "      source_lang = SRC_LANG,\n",
        "      target_lang = TGT_LANG\n",
        ")\n",
        "val['target_text'] = val['source_text']\n",
        "val['target_polarization'] = val['polarization']\n",
        "\n",
        "# Initialize dataset\n",
        "train_dataset = PolarPairsDataset(\n",
        "    source_texts = train['source_text'].tolist(),\n",
        "    target_texts = train['target_text'].tolist(),  # or different language\n",
        "    source_labels = train['polarization'].tolist(),\n",
        "    target_labels = train['target_polarization'].tolist(),  # labels for targets\n",
        "    tokenizer = tokenizer_param.get_value(),\n",
        "    max_length = max_length_param.get_value(),\n",
        "    subset_size = contrastive_subset_param.get_value()\n",
        ")\n",
        "\n",
        "val_dataset = PolarPairsDataset(\n",
        "    source_texts = val['source_text'].tolist(),\n",
        "    target_texts = val['target_text'].tolist(),  # or different language\n",
        "    source_labels = val['polarization'].tolist(),\n",
        "    target_labels = val['target_polarization'].tolist(),  # labels for targets\n",
        "    tokenizer = tokenizer_param.get_value(),\n",
        "    max_length = max_length_param.get_value(),\n",
        "    subset_size = contrastive_subset_param.get_value()\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = MultiLingualPolarPairsAlignment(\n",
        "    encoder_model_name=tokenizer_param.get_value(),\n",
        "    pretrained_encoder_name=f'finetuned_models/eng_{tokenizer_param.get_value()}',  # or 'bert-base-multilingual-cased'\n",
        "    num_labels=n_labels_param.get_value(),\n",
        "    alignment_normalization=alignment_normalization_param.get_value()\n",
        ")\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': model.encoder.parameters(), 'lr': encoder_lr_param.get_value()},\n",
        "    # {'params': model.translation_encoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.alignment_head.parameters(), 'lr': alignment_lr_param.get_value()},\n",
        "    {'params': model.classification_head.parameters(), 'lr': classification_lr_param.get_value()}\n",
        "])\n",
        "\n",
        "# Freeze encoders (only train alignment + classification heads)\n",
        "# for param in model.encoder.parameters():\n",
        "#     param.requires_grad = False\n",
        "# for param in model.translation_encoder.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "collator = PolarPairsCollator()\n",
        "\n",
        "# Train\n",
        "trainer = PolarPairsTrainer(\n",
        "    lambda_align = lambda_align_param.get_value(),\n",
        "    temperature=contrastive_temperature_param.get_value(),\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    compute_metrics = compute_metrics,\n",
        "    data_collator = collator,\n",
        "    optimizers = (optimizer, None)\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "target_results = trainer.evaluate()\n",
        "ft_results = {\n",
        "    'src_lang': SRC_LANG,\n",
        "    'tgt_lang': TGT_LANG,\n",
        "    'src_performance': target_results\n",
        "}\n",
        "_results_param = Parameter(ft_results, f\"language_fineTuning_results\", \"Performance\")\n",
        "experiment.add_params([_results_param])\n",
        "\n",
        "# ===== SAVE THE FINE-TUNED MODEL =====\n",
        "# save_path = f'finetuned_models/{TGT_LANG}-{SRC_LANG}_{tokenizer_param.get_value()}'\n",
        "# model.save_pretrained(save_path)\n",
        "# tokenizer.save_pretrained(save_path)\n",
        "# print(f\"Saved {TGT_LANG}-{SRC_LANG} model to {save_path}\")\n",
        "# ===================================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "lr4fuha5R-Ck",
        "outputId": "de3bfed7-3cd8-4794-cd50-f607ff485057"
      },
      "id": "lr4fuha5R-Ck",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Dataset: subtask1 | swa → eng\n",
            "============================================================\n",
            "Total samples: 6991\n",
            "Train set size: 5592 (80.0%)\n",
            "Validation set size: 1399 (20.0%)\n",
            "\n",
            "--- Polarization Distribution ---\n",
            "\n",
            "Train Set (Source):\n",
            "polarization\n",
            "0    2789\n",
            "1    2803\n",
            "Name: count, dtype: int64\n",
            "  Ratio: {0: 0.49874821173104433, 1: 0.5012517882689557}\n",
            "\n",
            "Validation Set (Source):\n",
            "polarization\n",
            "0    698\n",
            "1    701\n",
            "Name: count, dtype: int64\n",
            "  Ratio: {0: 0.498927805575411, 1: 0.501072194424589}\n",
            "\n",
            "Validation Set (Target):\n",
            "target_polarization\n",
            "0    929\n",
            "1    470\n",
            "Name: count, dtype: int64\n",
            "  Ratio: {0: 0.6640457469621158, 1: 0.3359542530378842}\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target distribution - Positive: 1903, Negative: 3689\n",
            "Target distribution - Positive: 701, Negative: 698\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='264' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [264/264 05:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.517200</td>\n",
              "      <td>0.701415</td>\n",
              "      <td>0.333810</td>\n",
              "      <td>0.334525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.651000</td>\n",
              "      <td>0.769670</td>\n",
              "      <td>0.558695</td>\n",
              "      <td>0.559013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.492800</td>\n",
              "      <td>0.545295</td>\n",
              "      <td>0.756093</td>\n",
              "      <td>0.756080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [175/175 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mDniEzmy--rl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDniEzmy--rl",
        "outputId": "60091036-b69e-49dc-f9d6-22f895002749"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amh_eval_results': {'eval_loss': 0.516802966594696,\n",
              "  'eval_f1_macro': 0.430401366353544,\n",
              "  'eval_f1_weighted': 0.6504416451040065,\n",
              "  'eval_runtime': 1.5397,\n",
              "  'eval_samples_per_second': 433.208,\n",
              "  'eval_steps_per_second': 54.557,\n",
              "  'epoch': 3.0},\n",
              " 'eng_eval_results': {'eval_loss': 0.46227237582206726,\n",
              "  'eval_f1_macro': 0.7995317226803696,\n",
              "  'eval_f1_weighted': 0.8141202139870262,\n",
              "  'eval_runtime': 1.3686,\n",
              "  'eval_samples_per_second': 471.293,\n",
              "  'eval_steps_per_second': 59.186,\n",
              "  'epoch': 3.0},\n",
              " 'hau_eval_results': {'eval_loss': 0.16815398633480072,\n",
              "  'eval_f1_macro': 0.8620973827395846,\n",
              "  'eval_f1_weighted': 0.9477186029288843,\n",
              "  'eval_runtime': 5.1359,\n",
              "  'eval_samples_per_second': 142.332,\n",
              "  'eval_steps_per_second': 17.913,\n",
              "  'epoch': 3.0},\n",
              " 'swa_eval_results': {'eval_loss': 0.5171564817428589,\n",
              "  'eval_f1_macro': 0.7659861363539869,\n",
              "  'eval_f1_weighted': 0.7660033537968377,\n",
              "  'eval_runtime': 3.0066,\n",
              "  'eval_samples_per_second': 465.309,\n",
              "  'eval_steps_per_second': 58.205,\n",
              "  'epoch': 3.0},\n",
              " 'language_fineTuning_results': {'src_lang': 'swa',\n",
              "  'tgt_lang': 'eng',\n",
              "  'src_performance': {'eval_loss': 0.5593292117118835,\n",
              "   'eval_f1_macro': 0.7417030428100538,\n",
              "   'eval_f1_weighted': 0.7416856224233133,\n",
              "   'eval_runtime': 8.638,\n",
              "   'eval_samples_per_second': 161.96,\n",
              "   'eval_steps_per_second': 20.259,\n",
              "   'epoch': 3.0}}}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "experiment.parameters['Performance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NfoQC4kMXiMe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfoQC4kMXiMe",
        "outputId": "1ac403ca-921e-4539-c863-2aedb37030c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to experiments/v1.1.0.yaml\n"
          ]
        }
      ],
      "source": [
        "# experiment.dir = f\"experiments/{experiment_version}.yaml\"\n",
        "experiment.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! zip NLP_LLMS_subtask1_experiments.zip experiments/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAcEXqC7TZXi",
        "outputId": "477feda9-6bc7-48ee-b9e8-2d885468dddb"
      },
      "id": "KAcEXqC7TZXi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: experiments/v1.0.0.yaml (deflated 61%)\n",
            "  adding: experiments/v1.0.1.yaml (deflated 60%)\n",
            "  adding: experiments/v1.0.2.yaml (deflated 61%)\n",
            "  adding: experiments/v1.0.3.yaml (deflated 60%)\n",
            "  adding: experiments/v1.0.4.yaml (deflated 61%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('NLP_LLMS_subtask1_experiments.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wNqX-6CPThxe",
        "outputId": "127e395a-d1c5-4f3d-e6e2-8cddecf97e34"
      },
      "id": "wNqX-6CPThxe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f79c46fa-2a05-4306-8ada-a1b1fdeba879\", \"NLP_LLMS_subtask1_experiments.zip\", 3884)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PolarPairsDataset(\n",
        "    source_texts = train['source_text'].tolist(),\n",
        "    target_texts = train['target_text'].tolist(),  # or different language\n",
        "    source_labels = train['polarization'].tolist(),\n",
        "    target_labels = train['target_polarization'].tolist(),  # labels for targets\n",
        "    tokenizer = tokenizer_param.get_value(),\n",
        "    max_length = max_length_param.get_value(),\n",
        "    subset_size = contrastive_subset_param.get_value()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-vwtN5Vtlu-",
        "outputId": "6059be65-ec18-4aee-9568-d8b62a6d174a"
      },
      "id": "l-vwtN5Vtlu-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target distribution - Positive: 1903, Negative: 3689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVEhaR6LtJ2u",
        "outputId": "e4b0e91c-4c85-40b9-8144-0a0a8d3d0e0f"
      },
      "id": "YVEhaR6LtJ2u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.PolarPairsDataset at 0x79ebd75a8740>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = train_dataset[[0, 1, 2, 3, 4]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "anYYO86ps9az",
        "outputId": "e8746ec1-8439-4a2f-dfed-c13a8d6e92ab"
      },
      "id": "anYYO86ps9az",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1665909660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-244310795.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0msource_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example['x_hat_input_ids'].size(), example['x_input_ids'].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA92dIjPtMrI",
        "outputId": "3fbc8c44-e2a4-4531-9088-e13e5cb047fa"
      },
      "id": "tA92dIjPtMrI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([20, 128]), torch.Size([128]))"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Qi53eheGIbu",
      "metadata": {
        "id": "5Qi53eheGIbu"
      },
      "source": [
        "# Subtask 2: Polarization Type Classification\n",
        "Multi-label classification to identify the target of polarization as one of the following categories: Gender/Sexual, Political, Religious, Racial/Ethnic, or Other.\n",
        "For this task we will load the data for subtask 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YaWIvnYv0rV2",
      "metadata": {
        "id": "YaWIvnYv0rV2"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('subtask2/train/eng.csv')\n",
        "val = pd.read_csv('subtask2/train/eng.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I6S2S6QBDKzw",
      "metadata": {
        "id": "I6S2S6QBDKzw"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # CHANGE THIS LINE: Use torch.float instead of torch.long for multi-label classification\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1_KYsG68nxI",
      "metadata": {
        "id": "u1_KYsG68nxI"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n",
        "dev_dataset = PolarizationDataset(val['text'].tolist(), val[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdiYJyr08bw2",
      "metadata": {
        "id": "cdiYJyr08bw2"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5, problem_type=\"multi_label_classification\") # 5 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ArVWKwze2mtS",
      "metadata": {
        "id": "ArVWKwze2mtS"
      },
      "outputs": [],
      "source": [
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qd3QPyfc2RKE",
      "metadata": {
        "id": "Qd3QPyfc2RKE"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 2: {eval_results['eval_f1_macro']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UL1uE8llIgTQ",
      "metadata": {
        "id": "UL1uE8llIgTQ"
      },
      "source": [
        "# Subtask 3: Manifestation Identification\n",
        "Multi-label classification to classify how polarization is expressed, with multiple possible labels including Vilification, Extreme Language, Stereotype, Invalidation, Lack of Empathy, and Dehumanization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCz20cgl-K3t",
      "metadata": {
        "id": "nCz20cgl-K3t"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('subtask3/train/eng.csv')\n",
        "val = pd.read_csv('subtask3/train/eng.csv')\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qs-UjVYsInpD",
      "metadata": {
        "id": "Qs-UjVYsInpD"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # CHANGE THIS LINE: Use torch.float instead of torch.long for multi-label classification\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-yxSaDCA9IMi",
      "metadata": {
        "id": "-yxSaDCA9IMi"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train[['vilification','extreme_language','stereotype','invalidation','lack_of_empathy','dehumanization']].values.tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val[['vilification','extreme_language','stereotype','invalidation','lack_of_empathy','dehumanization']].values.tolist(), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0VXqqqIH9A3M",
      "metadata": {
        "id": "0VXqqqIH9A3M"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6, problem_type=\"multi_label_classification\") # use 6 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QLubGtx988hm",
      "metadata": {
        "id": "QLubGtx988hm"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qEhm1TEv82mP",
      "metadata": {
        "id": "qEhm1TEv82mP"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 3: {eval_results['eval_f1_macro']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
        "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
        "5Qi53eheGIbu",
        "UL1uE8llIgTQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}