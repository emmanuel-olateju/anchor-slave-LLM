{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
      "metadata": {
        "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697"
      },
      "source": [
        "# Bert baseline for POLAR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
      "metadata": {
        "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this part of the starter notebook, we will take you through the process of all three Subtasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OjMTlSH2RBI1",
      "metadata": {
        "id": "OjMTlSH2RBI1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4TFIhGFZPWDa",
      "metadata": {
        "id": "4TFIhGFZPWDa"
      },
      "outputs": [],
      "source": [
        "! pip install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir experiments"
      ],
      "metadata": {
        "id": "xBYxURm-dTF-"
      },
      "id": "xBYxURm-dTF-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8RpSgNla3WQL",
      "metadata": {
        "cellView": "form",
        "id": "8RpSgNla3WQL"
      },
      "outputs": [],
      "source": [
        "# @title Experiment Class\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import yaml\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class Experiment:\n",
        "\n",
        "  def __init__(self, name, dir, description, baseline=None):\n",
        "    self.name = name\n",
        "    self.dir = dir\n",
        "    self.description = description\n",
        "    self.parameters = dict()\n",
        "    self.baseline = baseline\n",
        "\n",
        "  def update_param(self, parameter: 'Parameter'):\n",
        "    var_name = parameter.get_var_name()\n",
        "    parameter_class = parameter.get_parameter_class()\n",
        "    value = parameter.get_value()\n",
        "\n",
        "    assert isinstance(\n",
        "        value,\n",
        "        (\n",
        "            int, float, str, dict, list,\n",
        "            np.ndarray, torch.tensor\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if (parameter_class is None) or (parameter_class.lower() == 'global'):\n",
        "      self.parameters[var_name] = value\n",
        "      return\n",
        "\n",
        "    if parameter_class not in self.parameters:\n",
        "      self.parameters[parameter_class] = dict()\n",
        "    self.parameters[parameter_class][var_name] = value\n",
        "\n",
        "  def save(self):\n",
        "    experiment_dict = {\n",
        "        'name': self.name,\n",
        "        'baseline': self.baseline,\n",
        "        'description': self.description,\n",
        "        'parameters': self.parameters,\n",
        "    }\n",
        "\n",
        "    with open(self.dir, \"w\") as f:\n",
        "      yaml.dump(experiment_dict, f, default_flow_style=False)\n",
        "\n",
        "    print(f\"Model saved to {self.dir}\")\n",
        "\n",
        "  def add_params(self, parameters: List['Parameter']):\n",
        "\n",
        "    for parameter in parameters:\n",
        "      self.update_param(parameter)\n",
        "\n",
        "\n",
        "class Parameter:\n",
        "\n",
        "  def __init__(self, value, var_name, parameter_class):\n",
        "    self.__var_name = var_name\n",
        "    self.__value = value\n",
        "    self.__parameter_class = parameter_class\n",
        "\n",
        "  def get_var_name(self):\n",
        "    return self.__var_name\n",
        "\n",
        "  def get_parameter_class(self):\n",
        "    return self.__parameter_class\n",
        "\n",
        "  def get_value(self):\n",
        "    return self.__value\n",
        "\n",
        "  def set_value(self, value):\n",
        "    self.__value = value\n",
        "\n",
        "  def set_var_name(self, var_name):\n",
        "    self.__var_name = var_name\n",
        "\n",
        "  def set_parameter_class(self, parameter_class):\n",
        "    self.__parameter_class = parameter_class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
      "metadata": {
        "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8IzshMEjO74s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IzshMEjO74s",
        "outputId": "ae003e6b-adcb-4cbd-decb-37799641b3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tbAwUWN8X2JvXgdarjZ31f4XkqcpFVDk\n",
            "To: /content/dev_phase.zip\n",
            "100% 18.9M/18.9M [00:00<00:00, 214MB/s]\n"
          ]
        }
      ],
      "source": [
        "dev_phase_id = \"1tbAwUWN8X2JvXgdarjZ31f4XkqcpFVDk\"\n",
        "# subtask1_id = \"1q_I6dw9ZbCg3MbQ1wnC-419s2ocCyqaa\"\n",
        "# subtask2_id = \"1iHFDd_uihFi7vukWFq1hj32wfEH4dgBc\"\n",
        "# subtask3_id = \"1JA7_BbJDYORbmH06gWzz4-UhXgRBe1eI\"\n",
        "translated_tasks_id = \"1wHoKpZo8iMhHOm5TpvSS6Nr63Zk2w-P0\"\n",
        "\n",
        "! gdown --id $dev_phase_id\n",
        "! gdown --id $translated_tasks_id\n",
        "# ! gdown --id $subtask1_id\n",
        "# ! gdown --id $subtask2_id\n",
        "# ! gdown --id $subtask3_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5w0WSE89hdW",
      "metadata": {
        "id": "e5w0WSE89hdW"
      },
      "outputs": [],
      "source": [
        "! unzip dev_phase.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
      "metadata": {
        "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UkC2r47nManC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "UkC2r47nManC",
        "outputId": "83ca86d3-4fb3-4f20-a995-4e678d97d94a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/jxuulxpt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7aafe14169c0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Disable wandb logging for this script\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e749d08",
      "metadata": {
        "cellView": "form",
        "id": "8e749d08"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Class\n",
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
        "    self.texts=texts\n",
        "    self.labels=labels\n",
        "    self.tokenizer= tokenizer\n",
        "    self.max_length = max_length # Store max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "    encoding=self.tokenizer(text,truncation=True,padding=False,max_length=self.max_length,return_tensors='pt')\n",
        "\n",
        "    # Ensure consistent tensor conversion for all items\n",
        "    item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O_XiSY3I5jSH",
      "metadata": {
        "cellView": "form",
        "id": "O_XiSY3I5jSH"
      },
      "outputs": [],
      "source": [
        "# @title Functions\n",
        "\n",
        "# Define metrics function\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiTask Trainer Classes"
      ],
      "metadata": {
        "id": "yMxw5Vozdb_N"
      },
      "id": "yMxw5Vozdb_N"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    \"\"\"Dataset for both classification and translation tasks\"\"\"\n",
        "    def __init__(self, texts, labels, source_texts, target_texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.source_texts = source_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize for classification\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize source and target for translation\n",
        "        source_encoding = self.tokenizer(\n",
        "            self.source_texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            self.target_texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            'translation_input_ids': source_encoding['input_ids'].squeeze(0),\n",
        "            'translation_attention_mask': source_encoding['attention_mask'].squeeze(0),\n",
        "            'translation_labels': target_encoding['input_ids'].squeeze(0)\n",
        "        }\n",
        "\n",
        "\n",
        "class MultiTaskBERT(nn.Module):\n",
        "    \"\"\"BERT model with classification and translation heads\"\"\"\n",
        "    def __init__(self, model_name, num_labels, vocab_size, hidden_size=768):\n",
        "        super(MultiTaskBERT, self).__init__()\n",
        "\n",
        "        # Load pre-trained BERT\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Classification head (uses CLS token)\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, num_labels)\n",
        "        )\n",
        "\n",
        "        # Translation head (uses all token embeddings)\n",
        "        self.translation_head = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,\n",
        "                translation_input_ids=None, translation_attention_mask=None,\n",
        "                labels=None, translation_labels=None):\n",
        "\n",
        "        # Get BERT outputs for classification\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Classification: Use CLS token (first token)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
        "        classification_logits = self.classification_head(cls_output)\n",
        "\n",
        "        classification_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            classification_loss = loss_fct(classification_logits, labels)\n",
        "\n",
        "        # Translation: Use all token embeddings\n",
        "        translation_loss = None\n",
        "        translation_logits = None\n",
        "\n",
        "        if translation_input_ids is not None:\n",
        "            translation_outputs = self.bert(\n",
        "                input_ids=translation_input_ids,\n",
        "                attention_mask=translation_attention_mask\n",
        "            )\n",
        "            # Use all token embeddings for translation\n",
        "            token_embeddings = translation_outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "            translation_logits = self.translation_head(token_embeddings)  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "            if translation_labels is not None:\n",
        "                loss_fct = nn.CrossEntropyLoss(ignore_index=self.bert.config.pad_token_id)\n",
        "                translation_loss = loss_fct(\n",
        "                    translation_logits.view(-1, translation_logits.size(-1)),\n",
        "                    translation_labels.view(-1)\n",
        "                )\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = None\n",
        "        if classification_loss is not None and translation_loss is not None:\n",
        "            # You can weight these differently if needed\n",
        "            total_loss = classification_loss + translation_loss\n",
        "        elif classification_loss is not None:\n",
        "            total_loss = classification_loss\n",
        "        elif translation_loss is not None:\n",
        "            total_loss = translation_loss\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss,\n",
        "            'classification_loss': classification_loss,\n",
        "            'translation_loss': translation_loss,\n",
        "            'classification_logits': classification_logits,\n",
        "            'translation_logits': translation_logits\n",
        "        }\n",
        "\n",
        "\n",
        "# Custom Trainer for multi-task learning\n",
        "from transformers import Trainer\n",
        "\n",
        "class MultiTaskTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        outputs = model(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            translation_input_ids=inputs.get('translation_input_ids'),\n",
        "            translation_attention_mask=inputs.get('translation_attention_mask'),\n",
        "            labels=inputs.get('labels'),\n",
        "            translation_labels=inputs.get('translation_labels')\n",
        "        )\n",
        "\n",
        "        loss = outputs['loss']\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "LrOomH6NdadH"
      },
      "id": "LrOomH6NdadH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeJfGkKvdvg6"
      },
      "id": "PeJfGkKvdvg6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aIt64l96d4TR",
      "metadata": {
        "id": "aIt64l96d4TR"
      },
      "source": [
        "# Subtask 1 - Polarization detection\n",
        "\n",
        "This is a binary classification to determine whether a post contains polarized content (Polarized or Not Polarized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06pBeSoa7zYv",
      "metadata": {
        "id": "06pBeSoa7zYv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Experiment Definition & Parameter Setting\n",
        "\n",
        "import os\n",
        "\n",
        "languages = ['amh', 'eng', 'hau', 'swa'] # @param\n",
        "languages_param = Parameter(languages, 'language', None)\n",
        "\n",
        "model_name = \"RobertForSequenceClassification\" # @param {type: \"string\"}\n",
        "model_param = Parameter(model_name, 'model_name', None)\n",
        "\n",
        "n_labels = 2 # @param\n",
        "n_labels_param = Parameter(n_labels, 'n_labels', None)\n",
        "\n",
        "max_length = 128 # @param\n",
        "max_length_param = Parameter(max_length, 'max_length', 'Hyperparameter')\n",
        "\n",
        "tokenizer_choice = \"roberta-base-case\" # @param {type:\"string\"}\n",
        "tokenizer_param = Parameter(tokenizer_choice, 'tokenizer', 'Preprocessing')\n",
        "\n",
        "num_epochs = 3 # @param\n",
        "epochs_param = Parameter(num_epochs, 'epochs', 'Hyperparameter')\n",
        "\n",
        "lr = 2e-5 # @param\n",
        "lr_param = Parameter(lr, 'learning_rate', 'Hyperparameter')\n",
        "\n",
        "train_batch = 64 # @param\n",
        "train_batch_param = Parameter(train_batch, 'train_batch_size', 'Hyperparameter')\n",
        "\n",
        "eval_batch = 8 # @param\n",
        "eval_batch_param = Parameter(eval_batch, 'eval_batch_size', 'Hyperparameter')\n",
        "\n",
        "eval_strategy = \"epoch\" # @param {type: \"string\"}\n",
        "eval_strategy_param = Parameter(eval_strategy, 'eval_strategy', 'Hyperparameter')\n",
        "\n",
        "experiment_version = \"v1.1.0\" # @param {type: \"string\"}\n",
        "experiment_dir = f\"experiments/{experiment_version}.yaml\"\n",
        "if os.path.exists(experiment_dir):\n",
        "  raise ValueError(f\"Experiment {experiment_version} already exists\")\n",
        "experiment_description = \"Testingif data augmentation improves amharic, \" # @param {type: \"string\"}\n",
        "experiment_baseline = \"v1.0.0\" # @param {type: \"string\"}\n",
        "experiment = Experiment(\n",
        "    experiment_version,\n",
        "    experiment_dir,\n",
        "    experiment_description,\n",
        "    experiment_baseline\n",
        ")\n",
        "experiment.add_params([\n",
        "    languages_param, model_param, max_length_param,\n",
        "    tokenizer_param, epochs_param, lr_param,\n",
        "    train_batch_param, eval_batch_param,\n",
        "    eval_strategy_param\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JClhMb_kn-uQ",
      "metadata": {
        "id": "JClhMb_kn-uQ"
      },
      "source": [
        "## Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c97269",
      "metadata": {
        "id": "21c97269"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=f\"./\",\n",
        "        num_train_epochs=epochs_param.get_value(),\n",
        "        learning_rate=lr_param.get_value(),\n",
        "        per_device_train_batch_size=train_batch_param.get_value(),\n",
        "        per_device_eval_batch_size=eval_batch_param.get_value(),\n",
        "        eval_strategy=eval_strategy_param.get_value(),\n",
        "        save_strategy=\"no\",\n",
        "        logging_steps=100,\n",
        "        disable_tqdm=False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for language in languages_param.get_value():\n",
        "    train = pd.read_csv(f'subtask1_translated/train/{language}.csv')\n",
        "    val = pd.read_csv(f'subtask1_translated/dev/{language}.csv')\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_param.get_value())\n",
        "\n",
        "    # Create multi-task datasets\n",
        "    # Assuming you have 'source_text' and 'target_text' columns for translation\n",
        "    train_dataset = MultiTaskDataset(\n",
        "        texts=train['text'].tolist(),\n",
        "        labels=train['polarization'].tolist(),\n",
        "        source_texts=train['source_text'].tolist(),  # Add these columns\n",
        "        target_texts=train['target_text'].tolist(),   # Add these columns\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    val_dataset = MultiTaskDataset(\n",
        "        texts=val['text'].tolist(),\n",
        "        labels=val['polarization'].tolist(),\n",
        "        source_texts=val['source_text'].tolist(),\n",
        "        target_texts=val['target_text'].tolist(),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Initialize multi-task model\n",
        "    model = MultiTaskBERT(\n",
        "        model_name=tokenizer_param.get_value(),\n",
        "        num_labels=n_labels_param.get_value(),\n",
        "        vocab_size=tokenizer.vocab_size\n",
        "    )\n",
        "\n",
        "    # Optional: Freeze BERT layers and only train the heads\n",
        "    # for param in model.bert.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "    # Initialize the Multi-Task Trainer\n",
        "    trainer = MultiTaskTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Results on {language} validation set: {eval_results}\")\n",
        "\n",
        "    eval_results_param = Parameter(eval_results, f\"{language}_eval_results\", \"Performance\")\n",
        "    experiment.add_params([eval_results_param])\n",
        "\n",
        "    time.sleep(1)\n",
        "    clear_output()"
      ],
      "metadata": {
        "id": "080_c6JOisQq"
      },
      "id": "080_c6JOisQq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JcRDLDV2d2QA",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JcRDLDV2d2QA"
      },
      "outputs": [],
      "source": [
        "# for language in languages_param.get_value():\n",
        "\n",
        "#   train = pd.read_csv(f'subtask1/train/{language}.csv')\n",
        "#   val = pd.read_csv(f'subtask1/train/{language}.csv')\n",
        "#   # Load the tokenizer\n",
        "#   tokenizer = AutoTokenizer.from_pretrained(tokenizer_param.get_value())\n",
        "\n",
        "#   # Create datasets\n",
        "#   train_dataset = PolarizationDataset(train['text'].tolist(), train['polarization'].tolist(), tokenizer)\n",
        "#   val_dataset = PolarizationDataset(val['text'].tolist(), val['polarization'].tolist(), tokenizer)\n",
        "\n",
        "#   # Load the model\n",
        "#   model = AutoModelForSequenceClassification.from_pretrained(tokenizer_param.get_value(), num_labels=n_labels_param.get_value())\n",
        "\n",
        "#   # Initialize the Trainer\n",
        "#   trainer = Trainer(\n",
        "#       model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "#       args=training_args,                  # training arguments, defined above\n",
        "#       train_dataset=train_dataset,         # training dataset\n",
        "#       eval_dataset=val_dataset,            # evaluation dataset\n",
        "#       compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        "#       data_collator=DataCollatorWithPadding(tokenizer) # Data collator for dynamic padding\n",
        "#   )\n",
        "\n",
        "#   # Train the model\n",
        "#   trainer.train()\n",
        "\n",
        "#   eval_results = trainer.evaluate()\n",
        "#   print(f\"Macro F1 score on {language} validation set: {eval_results['eval_f1_macro']}\")\n",
        "\n",
        "#   eval_results_param = Parameter(eval_results, f\"{language}_eval_results\", \"Performance\")\n",
        "#   experiment.add_params([eval_results_param])\n",
        "\n",
        "#   time.sleep(1)\n",
        "#   clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mDniEzmy--rl",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDniEzmy--rl",
        "outputId": "4eaa96dd-8a31-464b-b03a-9cbf06767892"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'amh_eval_results': {'eval_loss': 0.5247534513473511,\n",
              "  'eval_f1_macro': 0.4304273504273504,\n",
              "  'eval_runtime': 12.3662,\n",
              "  'eval_samples_per_second': 269.444,\n",
              "  'eval_steps_per_second': 33.721,\n",
              "  'epoch': 3.0},\n",
              " 'eng_eval_results': {'eval_loss': 0.3241764307022095,\n",
              "  'eval_f1_macro': 0.8575853816004853,\n",
              "  'eval_runtime': 8.9028,\n",
              "  'eval_samples_per_second': 361.91,\n",
              "  'eval_steps_per_second': 45.267,\n",
              "  'epoch': 3.0},\n",
              " 'hau_eval_results': {'eval_loss': 0.20966212451457977,\n",
              "  'eval_f1_macro': 0.6924550413608028,\n",
              "  'eval_runtime': 21.9743,\n",
              "  'eval_samples_per_second': 166.149,\n",
              "  'eval_steps_per_second': 20.797,\n",
              "  'epoch': 3.0},\n",
              " 'swa_eval_results': {'eval_loss': 0.35833677649497986,\n",
              "  'eval_f1_macro': 0.8517672616733353,\n",
              "  'eval_runtime': 26.703,\n",
              "  'eval_samples_per_second': 261.806,\n",
              "  'eval_steps_per_second': 32.73,\n",
              "  'epoch': 3.0}}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment.parameters['Performance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NfoQC4kMXiMe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfoQC4kMXiMe",
        "outputId": "1a0bfc93-9ac1-4989-dc05-026b1aa77159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to experiments/v1.0.3.yaml\n"
          ]
        }
      ],
      "source": [
        "# experiment.dir = f\"experiments/{experiment_version}.yaml\"\n",
        "experiment.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! zip NLP_LLMS_subtask1_experiments.zip experiments/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAcEXqC7TZXi",
        "outputId": "20c704f4-787b-4e13-9705-40571952f15c"
      },
      "id": "KAcEXqC7TZXi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: experiments/v1.0.0.yaml (deflated 59%)\n",
            "  adding: experiments/v1.0.1.yaml (deflated 61%)\n",
            "  adding: experiments/v1.0.2.yaml (deflated 61%)\n",
            "  adding: experiments/v1.0.3.yaml (deflated 61%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('NLP_LLMS_subtask1_experiments.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wNqX-6CPThxe",
        "outputId": "136d9df3-ee7d-41ce-edff-84ec3b25c781"
      },
      "id": "wNqX-6CPThxe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e0a33fdc-a9c4-412d-b2c7-a0aa74c5ebac\", \"NLP_LLMS_subtask1_experiments.zip\", 2796)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Qi53eheGIbu",
      "metadata": {
        "id": "5Qi53eheGIbu"
      },
      "source": [
        "# Subtask 2: Polarization Type Classification\n",
        "Multi-label classification to identify the target of polarization as one of the following categories: Gender/Sexual, Political, Religious, Racial/Ethnic, or Other.\n",
        "For this task we will load the data for subtask 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YaWIvnYv0rV2",
      "metadata": {
        "id": "YaWIvnYv0rV2"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('subtask2/train/eng.csv')\n",
        "val = pd.read_csv('subtask2/train/eng.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I6S2S6QBDKzw",
      "metadata": {
        "id": "I6S2S6QBDKzw"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # CHANGE THIS LINE: Use torch.float instead of torch.long for multi-label classification\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1_KYsG68nxI",
      "metadata": {
        "id": "u1_KYsG68nxI"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n",
        "dev_dataset = PolarizationDataset(val['text'].tolist(), val[['gender/sexual','political','religious','racial/ethnic','other']].values.tolist(), tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdiYJyr08bw2",
      "metadata": {
        "id": "cdiYJyr08bw2"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5, problem_type=\"multi_label_classification\") # 5 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ArVWKwze2mtS",
      "metadata": {
        "id": "ArVWKwze2mtS"
      },
      "outputs": [],
      "source": [
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qd3QPyfc2RKE",
      "metadata": {
        "id": "Qd3QPyfc2RKE"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 2: {eval_results['eval_f1_macro']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UL1uE8llIgTQ",
      "metadata": {
        "id": "UL1uE8llIgTQ"
      },
      "source": [
        "# Subtask 3: Manifestation Identification\n",
        "Multi-label classification to classify how polarization is expressed, with multiple possible labels including Vilification, Extreme Language, Stereotype, Invalidation, Lack of Empathy, and Dehumanization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCz20cgl-K3t",
      "metadata": {
        "id": "nCz20cgl-K3t"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('subtask3/train/eng.csv')\n",
        "val = pd.read_csv('subtask3/train/eng.csv')\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qs-UjVYsInpD",
      "metadata": {
        "id": "Qs-UjVYsInpD"
      },
      "outputs": [],
      "source": [
        "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length # Store max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Ensure consistent tensor conversion for all items\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        # CHANGE THIS LINE: Use torch.float instead of torch.long for multi-label classification\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-yxSaDCA9IMi",
      "metadata": {
        "id": "-yxSaDCA9IMi"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create train and Test dataset for multilabel\n",
        "train_dataset = PolarizationDataset(train['text'].tolist(), train[['vilification','extreme_language','stereotype','invalidation','lack_of_empathy','dehumanization']].values.tolist(), tokenizer)\n",
        "val_dataset = PolarizationDataset(val['text'].tolist(), val[['vilification','extreme_language','stereotype','invalidation','lack_of_empathy','dehumanization']].values.tolist(), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0VXqqqIH9A3M",
      "metadata": {
        "id": "0VXqqqIH9A3M"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6, problem_type=\"multi_label_classification\") # use 6 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QLubGtx988hm",
      "metadata": {
        "id": "QLubGtx988hm"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Define metrics function for multi-label classification\n",
        "def compute_metrics_multilabel(p):\n",
        "    # Sigmoid the predictions to get probabilities\n",
        "    probs = torch.sigmoid(torch.from_numpy(p.predictions))\n",
        "    # Convert probabilities to predicted labels (0 or 1)\n",
        "    preds = (probs > 0.5).int().numpy()\n",
        "    # Compute macro F1 score\n",
        "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qEhm1TEv82mP",
      "metadata": {
        "id": "qEhm1TEv82mP"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_multilabel,  # Use the new metrics function\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Macro F1 score on validation set for Subtask 3: {eval_results['eval_f1_macro']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yMxw5Vozdb_N",
        "5Qi53eheGIbu",
        "UL1uE8llIgTQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}